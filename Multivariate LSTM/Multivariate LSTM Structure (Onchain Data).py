# -*- coding: utf-8 -*-
"""Onchain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N9_ysKC0Ocb4_bGFc0tNWikE_lnjiDHr
"""

pip install Quandl

import quandl
import math # Mathematical functions
from math import sqrt
import numpy as np # Fundamental package for scientific computing with Python
import pandas as pd # Additional functions for analysing and manipulating data
from datetime import date, timedelta, datetime # Date Functions
import matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data
import matplotlib.dates as mdates # Formatting dates
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error # Packages for measuring model performance / errors
from tensorflow.keras import Sequential # Deep learning library, used for neural networks
from tensorflow.keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers
from tensorflow.keras.callbacks import EarlyStopping # EarlyStopping during model training
from sklearn.preprocessing import MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data
import seaborn as sns # Visualization

sns.set_style('white', { 'axes.spines.right': False, 'axes.spines.top': False})

"""**1 Load the Time Series Data**"""

# Setting our API key
quandl.ApiConfig.api_key = 'PLbHumzEHFzBSiCbDCMo'

# Retrieving BTC Historical prices and onchain data
BTC_historical_price = quandl.get('BCHAIN/MKPRU', start_date = "2013-04-29", end_date = "2021-01-01")
BTC_difficulty = quandl.get('BCHAIN/DIFF', start_date = "2013-04-29", end_date = "2021-01-01")
BTC_wallet_users = quandl.get('BCHAIN/MWNUS', start_date = "2013-04-29", end_date = "2021-01-01")
BTC_average_block_size = quandl.get('BCHAIN/AVBLS', start_date = "2013-04-29", end_date = "2021-01-01")
BTC_hash_rate = quandl.get('BCHAIN/HRATE', start_date = "2013-04-29", end_date = "2021-01-01")
BTC_cost_per_transaction = quandl.get('BCHAIN/CPTRA', start_date = "2013-04-29", end_date = "2021-01-01")
BTC_transaction_confirmation_time = quandl.get('BCHAIN/ATRCT', start_date = "2013-04-29", end_date = "2021-01-01")
BTC_exchange_volume = quandl.get('BCHAIN/TRVOU', start_date = "2013-04-29", end_date = "2021-01-01")

# Merge the retrieved data
merged_data = pd.concat([BTC_historical_price, BTC_difficulty, BTC_wallet_users, BTC_average_block_size, BTC_hash_rate, BTC_cost_per_transaction, BTC_transaction_confirmation_time, BTC_exchange_volume], axis="columns")

# Renaming the columns
merged_data.columns = ['Historical Price', 'Difficulty', 'Wallet Users', 'Average Block Size', 'Hash Rate', 'Cost Per Transaction', 'Transaction Confirmation Time', 'Exchange Volume']


merged_data

# Checking for how many missing values
merged_data.isna().sum()

# Need to do something with missing values, we can assume the previous value due to the nature of our data so fill in with the previous value 
merged_data = merged_data.fillna(method='ffill')

merged_data.isna().sum()

"""**3 Feature Selection and Scaling**"""

# The Model refers to df and not merged_data we therefore make them se same
df = merged_data

"""**3.1 Selecting Features**"""

# Indexing Batches
train_df = df.sort_values(by=['Date']).copy()

# List of considered Features
FEATURES = ['Historical Price', 'Difficulty', 'Wallet Users', 'Cost Per Transaction', 'Exchange Volume'] # You can Hash out the features you don't need

print('FEATURE LIST')
print([f for f in FEATURES])

# Create the dataset with features and filter the data to the list of FEATURES
data = pd.DataFrame(train_df)
data_filtered = data[FEATURES]

# We add a prediction column and set dummy values to prepare the data for scaling
data_filtered_ext = data_filtered.copy()
data_filtered_ext['Prediction'] = data_filtered_ext['Historical Price'] # We use historical price

data_filtered_ext

data_filtered

"""**3.2 Scaling the Multivariate Input Data**"""

# Get the number of rows in the data
nrows = data_filtered.shape[0]

# Convert the data to numpy values
np_data_unscaled = np.array(data_filtered)
np_data = np.reshape(np_data_unscaled, (nrows, -1)) 
print(np_data.shape)

# Transform the data by scaling each feature to a range between 0 and 1
scaler = MinMaxScaler()
np_data_scaled = scaler.fit_transform(np_data_unscaled)

# Creating a separate scaler that works on a single column for scaling predictions
scaler_pred = MinMaxScaler()
df_Close = pd.DataFrame(data_filtered_ext['Historical Price']) # Just taking the price column and scale the data
np_Close_scaled = scaler_pred.fit_transform(df_Close)

"""**4 Transforming the Data**

**4.1 Split data into training and testing set**
"""

# Set the sequence length - this is the timeframe used to make a single prediction
sequence_length = 25

# Prediction Index
index_Close = data.columns.get_loc("Historical Price") # Finding the location of column for Historical Price, remember column 1 is the same as 0. therefore this returns 0 which means the first column 

# Split the training data into train and train data sets
# As a first step, we get the number of rows to train the model on 90% of the data 
train_data_len = math.ceil(np_data_scaled.shape[0] * 0.9)

# Create the training and test data
train_data = np_data_scaled[0:train_data_len, :]
test_data = np_data_scaled[train_data_len - sequence_length:, :]

"""**4.2 Define expression to divide the data**"""

# The RNN needs data with the format of [samples, time steps, features]
# Here, we create N samples, sequence_length time steps per sample, and n features
def partition_dataset(sequence_length, data):
    x, y = [], []
    data_len = data.shape[0]
    for i in range(sequence_length, data_len):
        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn
        y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction
    
    # Convert the x and y to numpy arrays
    x = np.array(x)
    y = np.array(y)
    return x, y

"""**4.3 Split the data**"""

# Generate training data and test data
x_train, y_train = partition_dataset(sequence_length, train_data)
x_test, y_test = partition_dataset(sequence_length, test_data)

# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# Validate that the prediction value and the input match up
# The last close price of the second input sample should equal the first prediction value
print(x_train[1][sequence_length-1][index_Close])
print(y_train[0])

"""**5. Train the Multivariate Prediction Model**"""

# Configure the neural network model
model = Sequential()

# Model with n_neurons = Sequence length * Number of variables (this is the way they determine the number of neurons in paper)
n_neurons = x_train.shape[1] * x_train.shape[2]

# Since we have a sequence of 25 days and our model has n variables we should have 25*n neurons
print(n_neurons, x_train.shape[1], x_train.shape[2])

model.add(LSTM(n_neurons, input_shape=(x_train.shape[1], x_train.shape[2]))) 
model.add(Dropout(0.3))
model.add(Dense(8))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Training the model
epochs = 5
batch_size = 25
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
history = model.fit(x_train, y_train, 
                    batch_size=batch_size, 
                    epochs=epochs,
                    validation_data=(x_test, y_test)
)
                    
                    #callbacks=[early_stop])

"""**Loss Graph**"""

# Plot training & validation loss values
fig, ax = plt.subplots(figsize=(16, 5), sharex=True)
sns.lineplot(data=history.history["loss"])
plt.title("Model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))
plt.legend(["Train", "Test"], loc="upper right")
plt.grid()
plt.show()

"""**6. Evaluate Model Performance**"""

# Get the predicted values
y_pred_scaled = model.predict(x_test)

# Unscale the predicted values
y_pred = scaler_pred.inverse_transform(y_pred_scaled)
y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))

# Meassures
mse = mean_squared_error(y_test_unscaled, y_pred)

rmse = sqrt(mean_squared_error(y_test_unscaled, y_pred))

mape = np.round((mean_absolute_percentage_error(y_test_unscaled, y_pred) * 100), 2)

mae = mean_absolute_error(y_test_unscaled, y_pred)

print("Mean Square Error (MSE):", mse)
print("Root Mean Square Error (RMSE):", rmse)
print("Mean Absolute Error (MSE):", mae)
print(f'Mean Absolute Percentage Error (MAPE): {mape} %')

"""**Line Plot**"""

plt.figure(figsize=(15, 8))
plt.plot(y_test_unscaled, color='black', label='Real')
plt.plot(y_pred, color='green', label='Predicted Price')
plt.title('Real vs Predicted BTC Price')
plt.xlabel('Days in Testing Period')
plt.ylabel('Price $')
plt.legend()
plt.show()

"""**GRID SEARCH**"""

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

# Create a function that will return a model with optimized parameters
def build_classifier(optimizer):
     grid_model = Sequential()
     grid_model.add(LSTM(n_neurons,input_shape=(x_train.shape[1], x_train.shape[2])))
     grid_model.add(Dropout(0.3))
     grid_model.add(Dense(8))
     grid_model.add(Dense(1))
     # Notice that below the optimizer is not adam but the function itself
     grid_model.compile(loss = 'mse',optimizer = optimizer, metrics = ['mean_squared_error'])
     return model

# Create a new model with the grid function built in 
grid_model = KerasClassifier(build_fn = build_classifier)

# Check which parameters you want the model to run through
parameters = {'batch_size' : [1,5,10,20,30,40,50],'epochs' : [5,10,20,30,40,50,60],'optimizer' : ['adam','Adadelta']}

# Now create the gridsearch, (n_jobs is either 1 for how many jobs to run in paraless, -1 means using all processors. cv is cross validation specified to 3 folds) 
grid_search  = GridSearchCV(estimator = grid_model, param_grid = parameters, n_jobs =-1, cv = 3)

# Run the gridsearch
grid_search = grid_search.fit(x_train,y_train)

# Tell me what the best parameters are
print("Best: %f using %s" % (grid_search.best_score_, grid_search.best_params_))

"""**Now use the new parameters in the model**"""

new_model = Sequential()
new_model.add(LSTM(n_neurons, input_shape=(x_train.shape[1], x_train.shape[2]))) 
new_model.add(Dropout(0.3))
new_model.add(Dense(8))
new_model.add(Dense(1))

# Compile the model
new_model.compile(optimizer='adam', loss='mse', metrics=['mean_squared_error'])

# CHANGE batch size and epochs according to resutls before running 
# Training the model
epochs = 10
batch_size = 50
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
history = new_model.fit(x_train, y_train, 
                    batch_size=batch_size, 
                    epochs=epochs,
                    validation_data=(x_test, y_test)
)
                    
                    #callbacks=[early_stop])

"""**New Loss graph**"""

# Plot training & validation loss values
fig, ax = plt.subplots(figsize=(16, 5), sharex=True)
sns.lineplot(data=history.history["loss"])
plt.title("Model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))
plt.legend(["Train", "Test"], loc="upper right")
plt.grid()
plt.show()

# Get the predicted values
y_pred_scaled = new_model.predict(x_test)

# Unscale the predicted values
y_pred = scaler_pred.inverse_transform(y_pred_scaled)

# Meassures
mse = mean_squared_error(y_test_unscaled, y_pred)

rmse = sqrt(mean_squared_error(y_test_unscaled, y_pred))

mape = np.round((mean_absolute_percentage_error(y_test_unscaled, y_pred) * 100), 2)

mae = mean_absolute_error(y_test_unscaled, y_pred)

print("Mean Square Error (MSE):", mse)
print("Root Mean Square Error (RMSE):", rmse)
print("Mean Absolute Error (MSE):", mae)
print(f'Mean Absolute Percentage Error (MAPE): {mape} %')

plt.figure(figsize=(15, 8))
plt.plot(y_test_unscaled, color='black', label='Real')
plt.plot(y_pred, color='green', label='Predicted Price')
plt.title('Real vs Predicted BTC Price')
plt.xlabel('Days in Testing Period')
plt.ylabel('Price $')
plt.legend()
plt.show()

"""**Now we will introduce the result by doing a Linear Regression do deal with the underfitting**"""

# Getting the test data 
BTC_test_data = quandl.get('BCHAIN/MKPRU', start_date = "2020-03-28", end_date = "2021-01-01")

# Remove the 'Date' index
BTC_test_data = BTC_test_data.reset_index(drop = True)
BTC_test_data.columns = ['Actual Price']
BTC_test_data

import statsmodels.api as sm

# predictors
X = y_pred

# response variable
y = BTC_test_data['Actual Price']

# fit the model
model = sm.OLS(y, X).fit()

# make predictions
predictions = model.predict(X)

# Meassures
mse = mean_squared_error(y_test_unscaled, predictions)

rmse = sqrt(mean_squared_error(y_test_unscaled, predictions))

mape = np.round((mean_absolute_percentage_error(y_test_unscaled, predictions) * 100), 2)

mae = mean_absolute_error(y_test_unscaled, predictions)

print("Mean Square Error (MSE):", mse)
print("Root Mean Square Error (RMSE):", rmse)
print("Mean Absolute Error (MSE):", mae)
print(f'Mean Absolute Percentage Error (MAPE): {mape} %')

plt.figure(figsize=(15, 8))
plt.plot(y, color='black', label='Real')
plt.plot(predictions, color='green', label='Onchain LR Model')
plt.plot(y_pred, color='red', label='Onchain Second Run')
plt.title('Real vs Predicted BTC')
plt.xlabel('Days in Testing Period')
plt.ylabel('BTC Price $')
plt.legend()
plt.show()